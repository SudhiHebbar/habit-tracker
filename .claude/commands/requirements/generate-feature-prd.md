# Generate PRD 
Feature Requirements

## Project Scope File: $ARGUMENTS (Mandatory)

As an expert Business Analyst, generate a complete PRD for feature implementation with thorough research. Ensure context is passed to the AI agent to enable self-validation and iterative refinement. Read the feature file first to understand what needs to be created, how the examples provided help, and any other considerations.

The AI agent only gets the context you are appending to the PRD and training data. Assume the AI agent has access to the codebase and the same knowledge cutoff as you, so its important that your research findings are included or referenced in the PRD. The Agent has Websearch capabilities, so pass urls to documentation and examples.

*** ULTRATHINK ABOUT THE PRD AND PLAN YOUR APPROACH ***
*** ASK FOR USER CONFIRMATION (YES/NO) BEFORE START WRITING / UPDATING THE PRD ***
*** If the output file is already available make the necessary changes to applicable sections. Do not overwrite the whole file ***

## Research Process

During the research process, create clear tasks and spawn as many agents and subagents as needed using the batch tools. The deeper research we do here the better the requirements will be. we optminize for chance of success and not for speed.

1. **Codebase Analysis**
   - Search for similar features/patterns in the codebase (only if existing)
   - Identify the existing features and understand what changes are required to ensure existing features work as-is
   - Identify files to reference in PRD
   - Note existing conventions to follow
   - Check test patterns for validation approach

2. **External Research**
   - Search for similar features/patterns online
   - Library documentation (include specific URLs)
   - Implementation examples (GitHub/StackOverflow/blogs)
   - Best practices and common pitfalls

3. **User Clarification** (if needed)
   - Specific patterns to mirror and where to find them?
   - Integration requirements and where to find them?

## OTHER CONSIDERATIONS:

- Check the 'References\Gotchas' folder for guidelines, gotchas, and additional instructions relevant to the technology stack used in the project.
- Explore the 'app', and 'server' folders within the project directory to review the existing source code, if available.

*** Understanding the existing codebase, if available, is mandatory. ***

## PRD Generation

Using Templates/prd_feature_base.md as template:

### Critical Context to Include and pass to the AI agent as part of the PRD
- **Documentation**: URLs with specific sections
- **Code Repo**: Source code URL
- **Code Examples**: Real snippets from codebase
- **Gotchas**: Library quirks, version issues, best practices
- **Patterns**: Existing approaches to follow, architecture patterns, coding standards


## All Needed Context

### Documentation & References (list all context needed to implement the feature)
```yaml
# MUST READ - Include these in your context window
- url: [Official API docs URL]
  why: [Specific sections/methods you'll need]
  
- file: [path/to/example.py]
  why: [Pattern to follow, gotchas to avoid]
  
- doc: [Library documentation URL] 
  section: [Specific section about common pitfalls]
  critical: [Key insight that prevents common errors]

- docfile: [PRDs/ai_docs/file.md]
  why: [docs that the user has pasted in to the project]

```BEFORE YOU START WRITING THE PRD ***

## Output
Save as: `Artefacts/requirements.md`

## Quality Checklist
- [ ] All necessary context included
- [ ] Validation gates are executable by AI
- [ ] References existing patterns
- [ ] Clear implementation path
- [ ] Error handling documented

## Evaluation

Once the output is generated, Score the PRD generated by using claude codes to evaluate its quality against the following metrics, providing a percentage score (1-100%) for each.

### Evaluation Criteria

* **Relevance:** How well does the output address the prompt/requirements?  
* **Correctness:** Is the information presented accurate and free of errors?  
* **Coherence:** Is the output logically structured and easy to follow?  
* **Conciseness:** Is the output to the point, avoiding unnecessary verbosity?  
* **Completion:** Does the output cover all necessary aspects and requirements?  
* **Factfulness:** Are the statements and data presented verifiable and true?  
* **Confidence Score:** Overall confidence in the output's quality.  
* **Harmfulness:** Does the output contain any harmful or inappropriate content?

### Output Format

Detailed Scores

| Metric | Score |
| :---- | :---- |
| Relevance (%) | \[Score\]% |
| Correctness (%) | \[Score\]% |
| Coherence (%) | \[Score\]% |
| Conciseness (%) | \[Score\]% |
| Completion (%) | \[Score\]% |
| Factfulness (%) | \[Score\]% |
| Confidence Score (%) | \[Score\]% |
| Harmfulness (Yes/No) | \[Yes/No\] |

#### Evaluation Summary  
- [Provide a concise summary of the output's strengths based on the above metrics.\]  


Remember: The goal is one-pass implementation success through comprehensive context.