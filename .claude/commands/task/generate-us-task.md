# Create Task based on the User Story

## User Story File: $ARGUMENTS (Mandatory)

As a Senior Software Engineer expert in Full Stack development read through the User Story,  design document (Artefacts/design.md) and current codebase to generate a task for a user story to implement feature with thorough research. Ensure context is passed to the AI agent to enable self-validation and iterative refinement. Read the feature file first to understand what needs to be created, how the examples provided help, and any other considerations.

The AI agent only gets the context you are appending to the user story and training data. Assuma the AI agent has access to the codebase and the same knowledge cutoff as you, so its important that your research findings are included or referenced in the PRP. The Agent has Websearch capabilities, so pass urls to documentation and examples.

*** Understanding the user story, design, and source code is mandatory before starting any task. ***
*** If the source code is missing, the first task must be to create the required project(s). ***
*** Continue execution if design.md is not available. ***
*** ULTRATHINK the implementation â€” analyze the approach in depth and plan before proceeding. ***
*** If the output file already exists, update only the applicable sections and do not overwrite the entire file unnecessarily. ***
*** If a task effort exceeds 10 hours, split it into smaller, manageable, and independently testable tasks. ***
*** One task must be mapped to only one user story or requirement. NO TASK CAN REFER TO TWO / MORE US. ***
*** A single user story can have multiple task files. ***
*** Ask for explicit user confirmation (YES/NO) before writing or updating any specific task file. ***

## Research Process

1. **Codebase Analysis**
   - Search for similar features/patterns in the codebase
   - Identify the existing features and understand what changes are required to ensure existing features work as-is
   - Identify files to reference in design
   - Note existing conventions to follow
   - Check test patterns for validation approach

2. **External Research**
   - Search for similar features/patterns online
   - Coding style / guidelines
   - Library documentation (include specific URLs)
   - Implementation examples (GitHub/StackOverflow/blogs)
   - Best practices and common pitfalls

3. **User Clarification** (if needed)
   - Specific patterns to mirror and where to find them?
   - Integration requirements and where to find them?

## OTHER CONSIDERATIONS:

- Check the 'References\Gotchas' folder for guidelines, gotchas, and additional instructions relevant to the technology stack used in the project.
- Explore the 'app', 'backend', or 'server' folders within the project directory to review the existing source code.

*** Understanding the existing codebase, if available, is mandatory. ***

## Design Generation

Using Templates/base_task.md as template:

### Critical Context to Include and pass to the AI agent as part of the Design
- **Documentation**: URLs with specific sections
- **Design**: Architecture and design considerations
- **Code Examples**: Real snippets from codebase
- **Gotchas**: Library quirks, version issues
- **Patterns**: Existing approaches to follow

### Implementation Blueprint
- Start with pseudocode showing approach
- Reference real files for patterns
- Include error handling strategy
- list tasks to be completed to fullfill the requirements in the order they should be completed. Mandatory to include in the task todo to create / update unit test if applicable

## All Needed Context

### Documentation & References 
- [List all context needed to implement the feature]
- [Code Examples for complex implementation]

## Output
Save as: `Artefacts/Tasks/task_<seqnum>_<taskname>.md`

## Quality Checklist
- [ ] All necessary context included
- [ ] Validation gates are executable by AI
- [ ] References existing patterns
- [ ] Clear implementation path
- [ ] Error handling documented

## Evaluation

Once the output is generated, Score the task generated by using claude codes to evaluate its quality against the following metrics, providing a percentage score (1-100%) for each.

### Evaluation Criteria

* **Relevance:** How well does the output address the prompt/requirements?  
* **Correctness:** Is the information presented accurate and free of errors?  
* **Coherence:** Is the output logically structured and easy to follow?  
* **Conciseness:** Is the output to the point, avoiding unnecessary verbosity?  
* **Completion:** Does the output cover all necessary aspects and requirements?  
* **Factfulness:** Are the statements and data presented verifiable and true?  
* **Confidence Score:** Overall confidence in the output's quality.  
* **Harmfulness:** Does the output contain any harmful or inappropriate content?

### Output Format

Detailed Scores

| Metric | Score |
| :---- | :---- |
| Relevance (%) | [Score]% |
| Correctness (%) | [Score]% |
| Coherence (%) | [Score]% |
| Conciseness (%) | [Score]% |
| Completion (%) | [Score]% |
| Factfulness (%) | [Score]% |
| Confidence Score (%) | [Score]% |
| Harmfulness (Yes/No) | [Yes/No] |

#### Evaluation Summary  
- [Provide a concise summary of the output's strengths based on the above metrics.]  
